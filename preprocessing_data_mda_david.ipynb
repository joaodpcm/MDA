{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "rgT_TlT9kZvT"
      ],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMHcTgojseqGsGn2vqWZSvS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joaodpcm/MDA/blob/joaodpcm-data/preprocessing_data_mda_david.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Exploration**\n",
        "\n",
        "---\n",
        "\n",
        "The notebook contains the preprocessing from the used data. It explains in detail \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zSF0saT-Vg_b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wIkuCGUGUa2i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Packages\n"
      ],
      "metadata": {
        "id": "pY1nyUChVJBz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing packages \n",
        "!pip install pandas-profiling\n",
        "!pip install ydata-profiling\n",
        "!pip install geopy\n",
        "!pip install featuretools\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import featuretools as ft\n",
        "import os\n",
        "import missingno as msno\n"
      ],
      "metadata": {
        "id": "qXZK-vbpVIto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clone repository"
      ],
      "metadata": {
        "id": "nHj8X3g1cRqg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Repository path\n",
        "repo_path = 'https://github.com/joaodpcm/MDAgit'\n",
        "#Accessing GitHub repository\n",
        "!git clone <https://github.com/joaodpcm/MDAgit>"
      ],
      "metadata": {
        "id": "mR9PDaWscT_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading data"
      ],
      "metadata": {
        "id": "YID7X-C7Vfn4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting google drive access\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "id": "38KhCJjITTyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting package for drive\n",
        "!pip install pyDrive\n",
        "from google.colab import files\n",
        "# Set path to the directory containing the meta dataset\n",
        "path_copy_meta = \"/content/drive/MyDrive/modern_data_analysis/data/meteo_data/01_Metadata_v2.csv\"\n",
        "df_meta = pd.read_csv(path_copy_meta)\n",
        "# Access to the year's quarter dataset \n",
        "path_copy_1 = \"/content/drive/MyDrive/modern_data_analysis/data/meteo_data/LC_2022Q3.csv\"\n",
        "path_copy_2 = \"/content/drive/MyDrive/modern_data_analysis/data/meteo_data/LC_2022Q3.csv\"\n",
        "path_copy_3 = \"/content/drive/MyDrive/modern_data_analysis/data/meteo_data/LC_2022Q3.csv\"\n",
        "path_copy_4 = \"/content/drive/MyDrive/modern_data_analysis/data/meteo_data/LC_2022Q3.csv\"\n",
        "# Loading each year's quarter dataset\n",
        "df_lc1 = pd.read_csv(path_copy_1)\n",
        "df_lc2 = pd.read_csv(path_copy_2)\n",
        "df_lc3 = pd.read_csv(path_copy_3)\n",
        "df_lc4 = pd.read_csv(path_copy_4)"
      ],
      "metadata": {
        "id": "MVijRJXcePtg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merging locations datasets\n",
        "\n",
        "The original datasets of the locations are concatenated for reaching more information about the complete year. Information about the quarter of the year is kept. "
      ],
      "metadata": {
        "id": "QU7n4V2OH1z-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add a column to identify the quarter\n",
        "df_lc1['quarter'] = 'Q1'\n",
        "df_lc2['quarter'] = 'Q2'\n",
        "df_lc3['quarter'] = 'Q3'\n",
        "df_lc4['quarter'] = 'Q4'\n",
        "\n",
        "# Concatenate the four datasets into a single dataframe\n",
        "df_lc = pd.concat([df_lc1, df_lc2, df_lc3, df_lc4])\n",
        "\n",
        "# # Print the final dataframe\n",
        "# print(df_lc)"
      ],
      "metadata": {
        "id": "mGO5iKXxBg1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_lc['id_new'] = np.arange(len(df_lc)) + 1"
      ],
      "metadata": {
        "id": "PFc5uvR-V4jO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_lc = df_lc.set_index('id_new')"
      ],
      "metadata": {
        "id": "PFqSEYqGV_ED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_lc.info()"
      ],
      "metadata": {
        "id": "nbirl1QgKqgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entity sets \n",
        "\n",
        "> In this part, a entity set from the datasets is created\n"
      ],
      "metadata": {
        "id": "uQ1CSCkY8c5T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_lc = df_lc.rename(columns={'id_new': 'lc_id'})\n",
        "df_meta = df_meta.rename(columns={'ID': 'meta_id'})"
      ],
      "metadata": {
        "id": "A5u7c9EtQ8XW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "es = ft.EntitySet(id='meteo')"
      ],
      "metadata": {
        "id": "vNBjAEe6Datn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "es = es.add_dataframe(\n",
        "    dataframe_name=\"locations\",\n",
        "    dataframe=df_lc,\n",
        "    index=\"lc_id\", \n",
        ")\n",
        "es = es.add_dataframe(\n",
        "    dataframe_name=\"meta\",\n",
        "    dataframe=df_meta,\n",
        "    index=\"meta_id\",\n",
        ")"
      ],
      "metadata": {
        "id": "X1vBOZlqUg9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "es = es.add_relationship(\"meta\", \"meta_id\", \"locations\", \"ID\")"
      ],
      "metadata": {
        "id": "cRYFf7WCXuPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataframes = {\n",
        "    \"locations\": (df_lc, \"lc_id\"),\n",
        "    \"meta\": (df_meta, \"meta_id\"),\n",
        "}"
      ],
      "metadata": {
        "id": "XuJJWm1McBGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "relationships = [\n",
        "    (\"meta\",\"meta_id\", \"locations\", \"ID\"),\n",
        "]"
      ],
      "metadata": {
        "id": "tg27u9OFceDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_matrix_locations, features_defs = ft.dfs(\n",
        "    dataframes=dataframes,\n",
        "    relationships=relationships,\n",
        "    target_dataframe_name=\"locations\",\n",
        ")\n",
        "feature_matrix_locations"
      ],
      "metadata": {
        "id": "ZrU9g-gxcqeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_entityset(entityset, sample_size):\n",
        "    \"\"\"\n",
        "    Get a sample of data from each dataframe in an entityset.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    entityset : featuretools.EntitySet\n",
        "        The entityset to get a sample from.\n",
        "    sample_size : int\n",
        "        The number of rows to sample from each dataframe.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    dict of pandas.DataFrame\n",
        "        A dictionary of dataframes, with the key being the dataframe name and the value being the sampled dataframe.\n",
        "    \"\"\"\n",
        "    sampled_dfs = {}\n",
        "    for entity in entityset.entities:\n",
        "        entity_df = entityset[entity.id]\n",
        "        sampled_df = sample(entity_df, sample_size)\n",
        "        sampled_dfs[entity.id] = sampled_df\n",
        "    return sampled_dfs"
      ],
      "metadata": {
        "id": "xNcTaJj_JYOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_entityset(entityset, sample_size=5000)"
      ],
      "metadata": {
        "id": "ym4FGOobJbrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merging metadata with locations dataset\n",
        "\n",
        "The information about the included features within the metadata is relevant for the main goal we established. For that reason the final locations dataset and the metadata will be concadenated."
      ],
      "metadata": {
        "id": "n_nF-EEiJJrH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# merge the two datasets by ID\n",
        "df_meteo = pd.merge(df_meta, df_lc, on='ID')\n",
        "\n",
        "# display the merged dataset\n",
        "print(df_meteo.head())"
      ],
      "metadata": {
        "id": "Ei4B-HANKstK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exporting meteo data"
      ],
      "metadata": {
        "id": "9Yr-CPYyNKr0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Exporting pre-cleaned meteo dataset\n",
        "\n",
        "# df_meteo.to_csv(\"merged_data.csv\", index=False)"
      ],
      "metadata": {
        "id": "mGjzJaLDLvED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df_meteo"
      ],
      "metadata": {
        "id": "NWcgKqW-hL52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data if needed (replace 0.5 with desired sample size)\n",
        "df_sampled = df.sample(frac=0.5, random_state=1)\n",
        "\n",
        "df = df_sampled"
      ],
      "metadata": {
        "id": "IROkLMHIsONj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3FtD7DHuW09"
      },
      "source": [
        "## Exploratory Data Analysis (EDA)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Standard EDA"
      ],
      "metadata": {
        "id": "mPgVVCLLzBIl"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgT_TlT9kZvT"
      },
      "source": [
        "#### Descriptive Statistics "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKYtUqa_km2t"
      },
      "outputs": [],
      "source": [
        "# Getting general information about the dataset \n",
        "df.info()\n",
        "# Reporting descriptive statistics for numerical features within dataset\n",
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating histograms and scatterplots for each feature"
      ],
      "metadata": {
        "id": "XRCI4XzM7Zln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDIL05NIvcM0"
      },
      "source": [
        "#### Exploring missingness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ojwxa_3Nv3Hn"
      },
      "outputs": [],
      "source": [
        "# Renaming missing values \n",
        "df = df.replace('', np.NaN)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the total missing values\n",
        "print(\"{} missing values within the dataset.\".format(df.isna().sum().sum())) "
      ],
      "metadata": {
        "id": "G5Qsa4l7Ri84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining function for recognizing missing values within features\n",
        "def features_missing_values(df):\n",
        "    missing = df.isnull().sum() # Identifying missing sum within each feature\n",
        "    missing_pct = (missing / len(df)) * 100 # Percentage of missingness compare to total n in dataset\n",
        "    missing_values = pd.concat([\n",
        "        missing, missing_pct.astype('int64')], # Getting type int64 for Percentage column \n",
        "        axis=1, \n",
        "        keys=[\n",
        "            'Total Missing', 'Percentage (%)']\n",
        "     ) # Creating dataframe with % of missing for each feature\n",
        "    missing_values.sort_values(by='Total Missing', ascending=False, inplace=True) # Sorting\n",
        "    missing_values.index.name = 'Feature' # Setting the index as 'Feature'\n",
        "    return missing_values # It returns an additional dataframe for the missing rate"
      ],
      "metadata": {
        "id": "D6LR-KzbLKVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying function for identifying missing values \n",
        "features_missing = features_missing_values(df)"
      ],
      "metadata": {
        "id": "HpkaD1WKLgVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(features_missing)"
      ],
      "metadata": {
        "id": "pAFgirMGBrD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def instances_missing_rate(df):\n",
        "    # Calculate missing value rate for each instance\n",
        "    missing_rate_instance = df.isna().sum(axis=1) / df.shape[1]\n",
        "    # Create dataframe with missing value rate for each instance\n",
        "    df_missing_rate = pd.DataFrame({'Instance': df.index, 'Missing_Rate': missing_rate_instance})\n",
        "    return df_missing_rate"
      ],
      "metadata": {
        "id": "UPQt48G4c4dI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "missing_rate_row = instances_missing_rate(df)"
      ],
      "metadata": {
        "id": "2d8edP5-JPWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(instances_missing_rate(df))"
      ],
      "metadata": {
        "id": "ltB0V19Jd6b_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Visualization of missingness patterns"
      ],
      "metadata": {
        "id": "dr_PxGyLa0-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import missingno as msno\n",
        "# Missingness matrix\n",
        "msno.matrix(df, figsize = (15, 13),  labels=True)"
      ],
      "metadata": {
        "id": "og3wNZHKY0AH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Missingness nullity correlation\n",
        "msno.heatmap(df)\n",
        "# Missingness in dendogram\n",
        "msno.dendrogram(df)"
      ],
      "metadata": {
        "id": "t3Q1WkR_r5TB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df is a pandas.DataFrame instance\n",
        "df_missing = df.iloc[:, [i for i, n in enumerate(np.var(df.isnull(), axis=0)) if n > 0]]\n",
        "corr_mat = df_missing.isnull().corr()"
      ],
      "metadata": {
        "id": "otgJ9zInbwgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corr_mat"
      ],
      "metadata": {
        "id": "TaYoXfHwrpOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoTiBxq0wUKB"
      },
      "source": [
        "For having a deeper understanding about the missingness within the dataset, please refer to the profile report on the training data."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exploring features"
      ],
      "metadata": {
        "id": "fK-plJpv5GEC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_histograms(data, folder_name):\n",
        "\n",
        "  # Create the directory if it doesn't exist\n",
        "  folder_path = \"/content/drive/MyDrive/modern_data_analysis/data/meteo_data\" + folder_name\n",
        "  !mkdir -p \"$folder_path\"\n",
        "  \n",
        "  # Generate histograms for each feature and save them to the folder\n",
        "  for column in df.columns:\n",
        "      fig, ax = plt.subplots(figsize=(8, 4))\n",
        "      df[column].hist(ax=ax, bins=20)\n",
        "      ax.set_title(column)\n",
        "      fig.savefig(folder_path + '/' + column + '.png')\n",
        "      plt.close(fig)"
      ],
      "metadata": {
        "id": "PWoBmi0m5Pt5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_histograms(df, 'histograms_folder')"
      ],
      "metadata": {
        "id": "WwaqCRd29qh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.kdeplot(data=df, x=\"target\")"
      ],
      "metadata": {
        "id": "uA4mky2vmaMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WZ_qMKC77Jo"
      },
      "source": [
        "#### Correlation among features"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Definying a function for correlation identification with heatmap visualization\n",
        "def correlation_heatmap(df):\n",
        "    # Getting correlation matrix\n",
        "    corr_matrix = df.corr()\n",
        "    # Print correlation matrix\n",
        "    print(\"Correlation Matrix:\\n\", corr_matrix.to_string())\n",
        "    # Create heatmap plot\n",
        "    plt.figure(figsize=(10, 8)) # Suggested for plotting the correlation heatmap\n",
        "    sns.heatmap(corr_matrix, cmap='coolwarm', annot=True, vmin=-1, vmax=1, fmt='.1f')\n",
        "    # Show the plot\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "E_NN3z0zJHh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting and visualizing correlations with a heatmap and the correlation matrix\n",
        "correlation_heatmap(df)"
      ],
      "metadata": {
        "id": "1eO54DKoJby_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Identifying outliers"
      ],
      "metadata": {
        "id": "xZUz_bCbgBSK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Getting information about the features"
      ],
      "metadata": {
        "id": "WlOna-XjjYVw"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h9itUix6Uzmc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Locations"
      ],
      "metadata": {
        "id": "LreWbzkXgF0-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Heatmap for locations \n",
        "# # Create a heatmap\n",
        "# plt.figure(figsize=(10, 8))\n",
        "# heatmap = sns.kdeplot(data=df, x='property_lat', y='property_lon', hue='target', cmap='Reds', fill=True)\n",
        "# # # Set the x-axis and y-axis limits to 5 units\n",
        "# # plt.xlim(-4, 4)\n",
        "# # plt.ylim(-4, 4)\n",
        "\n",
        "# # Show the plot\n",
        "# plt.show()\n"
      ],
      "metadata": {
        "id": "IprPQ-IyjYEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "\n",
        "# Compute the linkage matrix using Ward's method\n",
        "linkage_matrix = linkage(df[['Fedu']], method='ward')\n",
        "\n",
        "# Plot the dendrogram\n",
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "dendrogram(linkage_matrix, ax=ax)\n",
        "plt.xlabel('Sample index')\n",
        "plt.ylabel('Distance')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "4CCiwi_9jTy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the linkage matrix using Ward's method\n",
        "linkage_matrix = linkage(df[['Medu']], method='ward')\n",
        "\n",
        "# Plot the dendrogram\n",
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "dendrogram(linkage_matrix, ax=ax)\n",
        "plt.xlabel('Sample index')\n",
        "plt.ylabel('Distance')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "HkMqMWV3tAy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "\n",
        "# Extract the 2 features for clustering\n",
        "X = df[['Fedu', 'Medu']].values\n",
        "\n",
        "# Calculate the linkage matrix using Ward's method\n",
        "Z = linkage(X, 'ward')\n",
        "\n",
        "# Plot the dendrogram\n",
        "plt.figure(figsize=(10, 5))\n",
        "dendrogram(Z)\n",
        "plt.title('Hierarchical Clustering Dendrogram')\n",
        "plt.xlabel('Data points')\n",
        "plt.ylabel('Distance')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ieP2HphOtJCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Extract the categorical feature\n",
        "cat_feature = df['Fedu']\n",
        "\n",
        "# Transform the categorical feature using one-hot encoding\n",
        "encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
        "cat_feature_encoded = encoder.fit_transform(cat_feature.to_numpy().reshape(-1, 1))\n",
        "\n",
        "# Perform hierarchical clustering with 2 clusters\n",
        "n_clusters = 2\n",
        "clustering = AgglomerativeClustering(n_clusters=n_clusters)\n",
        "clusters = clustering.fit_predict(cat_feature_encoded)\n",
        "\n",
        "# Print the resulting clusters\n",
        "print(clusters)"
      ],
      "metadata": {
        "id": "sH6Ma9x-vJFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pyu5N5c47xoX"
      },
      "source": [
        "### Using profiling report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKgwjaAru_9i"
      },
      "outputs": [],
      "source": [
        "# Profile report\n",
        "from ydata_profiling import ProfileReport\n",
        "from ydata_profiling.utils.cache import cache_file\n",
        "profile1 = ProfileReport(\n",
        "    df, title=\"Exploratory Data Analysis\", html={\"style\": {\"full_width\": True}}, sort=None\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Report using profiling \n",
        "profile1.to_notebook_iframe()\n",
        "profile1.to_file(\"EDA_{course}.html\")"
      ],
      "metadata": {
        "id": "-JKkGs6oRKz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "KquXT0MqAv5L"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqAjkb6Lf45n"
      },
      "source": [
        "## Data Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cleaning errors and duplications"
      ],
      "metadata": {
        "id": "DHXL4rlcgoqj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining function for removing possible duplicated features in a data set\n",
        "def remove_duplicate_features(data):\n",
        "    duplicates = set() # Empty set\n",
        "    kept_features = [] # Empty list for kept features\n",
        "    removed_features = [] # Empty list for removed features\n",
        "  \n",
        "    for i, column1 in enumerate(data.columns): # Loop over each column\n",
        "        for j, column2 in enumerate(data.columns[i+1:], i+1): # Loop after column1\n",
        "            # Checking if column1 and column2 are equal\n",
        "            if df[column1].equals(data[column2]):\n",
        "                # If they are equal, add column2 to the duplicates set\n",
        "                duplicates.add(column2)\n",
        "                # Get the kept feature, which is the first column of the pair\n",
        "                kept_feature = data.columns[i]\n",
        "                kept_features.append(kept_feature) # list of the kept features\n",
        "                # Add removed features to a list\n",
        "                removed_features.append(column2)\n",
        "  \n",
        "    df_dropped = data.drop(columns=duplicates) # df dropping the duplicates\n",
        "    \n",
        "    # Print a message indicating the names of the dropped columns\n",
        "    if len(duplicates) > 0:\n",
        "        # Combine the duplicate columns and their kept features\n",
        "        removed_features_text = \", \".join([f\"{removed_features[i]}\" for i in range(len(removed_features))])\n",
        "        kept_features_text = \", \".join(kept_features)\n",
        "        print(f\"The following features were removed due to duplicates: {removed_features_text}\")\n",
        "        print(f\"Kept features: {kept_features_text}.\")\n",
        "    else:\n",
        "        print(\"No duplicate features were found. No feature removed\")\n",
        "    return df_dropped"
      ],
      "metadata": {
        "id": "FOOtQjzaP30f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing duplicated features\n",
        "df = remove_duplicate_features(df)"
      ],
      "metadata": {
        "id": "hDawiUa1HQrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify duplicate rows\n",
        "duplicates = df.duplicated()\n",
        "\n",
        "# Print the number of duplicates\n",
        "print('Number of duplicates:', duplicates.sum())\n",
        "\n",
        "# Drop the duplicate rows\n",
        "df.drop_duplicates(inplace=True)\n",
        "\n",
        "# Verify that duplicates have been removed\n",
        "print('Number of duplicates after removal:', df.duplicated().sum())"
      ],
      "metadata": {
        "id": "9IMNw1x6SUtX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dealing with missigness"
      ],
      "metadata": {
        "id": "KI4RN-sf4z8E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5E-ZiLf1dT7X"
      },
      "outputs": [],
      "source": [
        "# Definying a function for removing features with missing rate greater than 90% \n",
        "def remove_missing_features(data, threshold=0.9):\n",
        "    missing_perc = data.isnull().sum() / len(data) * 100 # It calculates the percentage of missingness within each feature\n",
        "    high_missing_features = missing_perc[missing_perc >= threshold*100].index.tolist() # Object that includes missing values percentage within features greater than 90%\n",
        "    # If no high missing features, return the original DataFrame\n",
        "    if not high_missing_features:\n",
        "        print(\"No features with missingness rate greater than {}% found. The default threshold is 90%.\".format(threshold*100))\n",
        "        return data\n",
        "    # Remove the identified features with missingness greater than 90% from the DataFrame\n",
        "    data_removed = data.drop(high_missing_features, axis=1)\n",
        "    # Print the names of the removed features\n",
        "    print(\"The following features have been removed due to high missingness:\")\n",
        "    for feature in high_missing_features:\n",
        "        print(\"- {}\".format(feature))\n",
        "    return data_removed"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying the function to remove features with missing rate greater than 90%\n",
        "df = remove_missing_features(df)"
      ],
      "metadata": {
        "id": "0v1DbLQGDL0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining a function for removing instances with missing rate greater than 90%"
      ],
      "metadata": {
        "id": "2s1JLwOGVLhJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dealing with outliers"
      ],
      "metadata": {
        "id": "j30WWcvYCmo1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "\n",
        "\n",
        "# Separate the categorical and numerical features\n",
        "categorical_features = df.select_dtypes(include=['object']).columns\n",
        "numerical_features = df.select_dtypes(include=['float64', 'int64']).columns\n",
        "\n",
        "# Perform one-hot encoding on the categorical features\n",
        "encoder = OneHotEncoder(sparse=False, handle_unknown='ignore', drop='first')\n",
        "onehot = encoder.fit_transform(df[categorical_features])\n",
        "onehot_labels = encoder.get_feature_names_out(categorical_features)\n",
        "\n",
        "# Create a new DataFrame with the one-hot encoded features\n",
        "df_onehot = pd.DataFrame(data=onehot, columns=onehot_labels)\n",
        "\n",
        "# Concatenate the one-hot encoded features with the numerical features\n",
        "df = pd.concat([df_onehot, df[numerical_features]], axis=1)\n",
        "\n",
        "# Categorization for applying LOC\n",
        "lof = LocalOutlierFactor(n_neighbors=2, contamination=0.02)\n",
        "y_pred = lof.fit_predict(df)\n",
        "\n",
        "# create a new column in the original DataFrame to store outlier labels\n",
        "df['is_outlier'] = y_pred\n",
        "\n",
        "# save the merged DataFrame to a new CSV file\n",
        "df.to_csv('outliers.csv', index=False)\n"
      ],
      "metadata": {
        "id": "kRoAl1SzwYnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/modern_data_analysis/data/meteo_data/outliers.csv')"
      ],
      "metadata": {
        "id": "J4LQbKstbuUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZDwZ6yd9CLZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# count the unique values of a feature\n",
        "unique_counts = df['is_outlier'].value_counts()\n",
        "\n",
        "# print the unique value counts\n",
        "print(unique_counts)"
      ],
      "metadata": {
        "id": "aOfalX_dcBDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definying a function for correlation identification with heatmap visualization\n",
        "def correlation_heatmap(df):\n",
        "    # Getting correlation matrix\n",
        "    corr_matrix = df.corr()\n",
        "    # Print correlation matrix\n",
        "    print(\"Correlation Matrix:\\n\", corr_matrix.to_string())\n",
        "    # Create heatmap plot\n",
        "    plt.figure(figsize=(20, 18)) # Suggested for plotting the correlation heatmap\n",
        "    sns.heatmap(corr_matrix, cmap='coolwarm', annot=True, vmin=-1, vmax=1, fmt='.1f')\n",
        "    # Show the plot\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "4eNujccmhyn-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = correlation_heatmap(df)"
      ],
      "metadata": {
        "id": "vVp8PLi9h2qj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cItfHIvViOyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "nzlV_26TAv20"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VgQwwpTkAss4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b48Wy-txvrrk"
      },
      "source": [
        "When using Google Collab, to the get the Exploratory Data Analysis report is necessary to store a html file and then download it. It's because of the size of the file."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "2XrH8wiKjQyN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pKbhQhdYjEkS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}